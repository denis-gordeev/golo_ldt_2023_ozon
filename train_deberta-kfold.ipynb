{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -xvf hackathon_files_for_participants_ozon.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install catboost\n",
    "# !pip install pandas\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install seaborn\n",
    "# !pip install pyarrow\n",
    "# !pip install transformers\n",
    "# !pip install setfit\n",
    "# !pip install torch\n",
    "# !pip install -U protobuf==3.20\n",
    "# !pip uninstall -y transformers accelerate\n",
    "# !pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5 (setup_file_logger):\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/python3.10/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/envs/python3.10/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1801/3159764320.py\", line 10, in setup_file_logger\n",
      "NameError: name 'MODEL_CKPT' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background logger started\n"
     ]
    }
   ],
   "source": [
    "# COMMENT: Just curios, why this logger? \n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "from threading import Thread\n",
    "# Cloudwatch logging\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def setup_file_logger():\n",
    "    log_file = \"{}-{}.log\".format(MODEL_CKPT.replace(\"/\", \"_\"), str(datetime.datetime.now()))\n",
    "    hdlr = logging.FileHandler(log_file)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr) \n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "def log(message, type='info'):\n",
    "    #outputs to Jupyter console\n",
    "    print('{} {}'.format(datetime.datetime.now(), message))\n",
    "    #outputs to file\n",
    "    if type == 'info':\n",
    "        logger.info(message)\n",
    "    elif type == 'warning':\n",
    "        logger.warning(message)\n",
    "    elif type == 'error':\n",
    "        logger.error(message)\n",
    "    elif type == 'critical':\n",
    "        logger.critical(message)\n",
    "\n",
    "threaded_logging = Thread(target=setup_file_logger)\n",
    "threaded_logging.start()\n",
    "threaded_logging.join()\n",
    "print(\"Background logger started\")\n",
    "\n",
    "# Cloudwatch logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# stdout logging\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May 29 12:43:22 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0 Off |                  N/A |\n",
      "|  0%   53C    P5    38W / 170W |      1MiB / 12288MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T06:10:23.678400Z",
     "start_time": "2023-05-19T06:10:16.503064Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from catboost.utils import eval_metric\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          EarlyStoppingCallback, IntervalStrategy, Trainer,\n",
    "                          TrainingArguments)\n",
    "from transformers import logging as transformers_logging\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(features):\n",
    "    features = features[[\"name1\", \"name2\", \"target\"]]\n",
    "    features[\"text\"] = features[\"name1\"] + \"[SEP]\" + features[\"name2\"]\n",
    "    features[\"label\"] = features[\"target\"].astype(int)\n",
    "    features = features[[\"text\", \"label\"]]\n",
    "    features = features.drop_duplicates()\n",
    "    features = features.reset_index(drop=True)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], max_length=256, padding=\"longest\", truncation=True\n",
    "    )\n",
    "\n",
    "def get_hf_dataset(train_df, val_df):\n",
    "    hf_datasets = dict()\n",
    "    for key, value in [(\"train\", train_df), (\"val\", val_df)]:\n",
    "        dataset = Dataset.from_pandas(value, split=\"key\")\n",
    "        hf_datasets[key] = dataset.map(tokenize_function, batched=True)\n",
    "    return hf_datasets\n",
    "\n",
    "\n",
    "def tokenize(message, device=\"cuda\", **kwargs):\n",
    "    text_enc = tokenizer.encode_plus(\n",
    "        message,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=CONFIG[\"max_token_count\"],\n",
    "        padding=\"max_length\",\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    input_ids = torch.tensor(text_enc[\"input_ids\"], dtype=torch.long).to(device)\n",
    "    attention_mask = torch.tensor(text_enc[\"attention_mask\"], dtype=torch.long).to(\n",
    "        device\n",
    "    )\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "    \n",
    "def inference(message, **kwargs):\n",
    "    input_ids, attention_mask = tokenize(message, **kwargs)\n",
    "    preds = model(input_ids, attention_mask)\n",
    "    logits = sigmoid(preds[\"logits\"])[0].detach().cpu().numpy()\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT = \"microsoft/deberta-v3-base\"\n",
    "MODEL_PATH = \"/predefined_classes_best\"\n",
    "\n",
    "CONFIG = {\"max_token_count\": 512}\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/envs/python3.10/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_CKPT, use_fast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"A function for computing metrics for multi-label HF-classifier models\n",
    "\n",
    "    Args:\n",
    "        eval_pred (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    pred_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    f1_results = f1_score(labels, pred_labels)\n",
    "    precision_results = precision_score(labels, pred_labels)\n",
    "    recall_results = recall_score(labels, pred_labels)\n",
    "\n",
    "    output = {\"f1\": f1_results, \"precision\": precision_results, \"recall\": recall_results}\n",
    "    logger.info(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "\n",
    "# for fold in range(5):\n",
    "#     train = pd.read_parquet(f\"/root/fold_{fold}_train.parquet\")\n",
    "#     test = pd.read_parquet(f\"/root/fold_{fold}_test.parquet\")\n",
    "#     val = pd.read_parquet(f\"/root/fold_{fold}_val.parquet\")\n",
    "    \n",
    "#     train = process_df(train)\n",
    "#     test = process_df(test)\n",
    "#     val = process_df(val)\n",
    "        \n",
    "#     train_texts = set(train[\"text\"].values)\n",
    "#     val = val[val[\"text\"].apply(lambda x: x not in train_texts)]\n",
    "#     test = test[test[\"text\"].apply(lambda x: x not in train_texts)]\n",
    "\n",
    "#     best_model_name = MODEL_PATH\n",
    "#     hf_datasets = get_hf_dataset(train, val)\n",
    "#     class_names = [0, 1]\n",
    "#     id2label = {i: label for i, label in enumerate(class_names)}\n",
    "#     if \"mapping\" not in CONFIG:\n",
    "#         CONFIG['mapping'] = {'id_to_anecdote': id2label}\n",
    "#     # init model\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         MODEL_CKPT,\n",
    "#         num_labels=2,\n",
    "#         problem_type=\"single_label_classification\",\n",
    "#         ).to('cuda')\n",
    "#     model.config.id2label = id2label\n",
    "\n",
    "#     args = TrainingArguments(\n",
    "#         output_dir=\"predefined_model_new\",\n",
    "#         evaluation_strategy = \"epoch\",\n",
    "#         save_strategy = \"epoch\",\n",
    "#         learning_rate=1e-4,\n",
    "#         per_device_train_batch_size=BATCH_SIZE,\n",
    "#         per_device_eval_batch_size=BATCH_SIZE,\n",
    "#         #num_train_epochs=10000,\n",
    "#         num_train_epochs=1,\n",
    "#         weight_decay=0.01,\n",
    "#         save_total_limit=1,\n",
    "#         metric_for_best_model = 'f1',\n",
    "#         load_best_model_at_end=True,\n",
    "#         fp16=True,\n",
    "#     #         report_to=\"tensorboard\"\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model,\n",
    "#         args,\n",
    "#         train_dataset=hf_datasets[\"train\"],\n",
    "#         eval_dataset=hf_datasets[\"val\"],\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         tokenizer=tokenizer,\n",
    "#         callbacks = [EarlyStoppingCallback(early_stopping_patience=1)]\n",
    "#     )\n",
    "\n",
    "#     prev_score = -1\n",
    "#     best_model_checkpoint = None\n",
    "\n",
    "#     trainer.evaluate()\n",
    "#     trainer.train()\n",
    "#     results = trainer.evaluate()\n",
    "\n",
    "#     trainer.save_model(f\"{best_model_name}_{fold}\")\n",
    "    \n",
    "#     del trainer\n",
    "#     del model\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "\n",
    "# for fold in range(5):\n",
    "#     test = pd.read_parquet(f\"/root/fold_{fold}_test.parquet\")\n",
    "#     test = process_df(test)\n",
    "#     test = test[test[\"text\"].apply(lambda x: x not in train_texts)]\n",
    "\n",
    "#     best_model_name = MODEL_PATH\n",
    "#     hf_datasets = get_hf_dataset(test[:10], test)\n",
    "#     class_names = [0, 1]\n",
    "#     id2label = {i: label for i, label in enumerate(class_names)}\n",
    "#     if \"mapping\" not in CONFIG:\n",
    "#         CONFIG['mapping'] = {'id_to_anecdote': id2label}\n",
    "#     # init model\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         f\"/predefined_classes_best_{fold}\",\n",
    "#         num_labels=2,\n",
    "#         problem_type=\"single_label_classification\",\n",
    "#         ).to('cuda')\n",
    "#     model.config.id2label = id2label\n",
    "\n",
    "#     args = TrainingArguments(\n",
    "#         output_dir=\"predefined_model_new\",\n",
    "#         evaluation_strategy = \"epoch\",\n",
    "#         save_strategy = \"epoch\",\n",
    "#         learning_rate=1e-4,\n",
    "#         per_device_train_batch_size=BATCH_SIZE,\n",
    "#         per_device_eval_batch_size=BATCH_SIZE,\n",
    "#         #num_train_epochs=10000,\n",
    "#         num_train_epochs=1,\n",
    "#         weight_decay=0.01,\n",
    "#         save_total_limit=1,\n",
    "#         metric_for_best_model = 'f1',\n",
    "#         load_best_model_at_end=True,\n",
    "#         fp16=True,\n",
    "#     #         report_to=\"tensorboard\"\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model,\n",
    "#         args,\n",
    "#         train_dataset=hf_datasets[\"train\"],\n",
    "#         eval_dataset=hf_datasets[\"val\"],\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         tokenizer=tokenizer,\n",
    "#         callbacks = [EarlyStoppingCallback(early_stopping_patience=1)]\n",
    "#     )\n",
    "\n",
    "#     prev_score = -1\n",
    "#     best_model_checkpoint = None\n",
    "\n",
    "#     trainer.evaluate()\n",
    "    \n",
    "#     del trainer\n",
    "#     del model\n",
    "#     torch.cuda.empty_cache()\n",
    "#     gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "def pr_auc_macro(\n",
    "    df: pd.DataFrame,\n",
    "    prec_level: float = 0.75,\n",
    "    cat_column: str = \"cat3_grouped\"\n",
    ") -> float:\n",
    "\n",
    "    y_true = df[\"label\"]\n",
    "    y_pred = df[\"scores\"]\n",
    "    categories = df[cat_column]\n",
    "\n",
    "    weights = []\n",
    "    pr_aucs = []\n",
    "\n",
    "    unique_cats, counts = np.unique(categories, return_counts=True)\n",
    "\n",
    "    # calculate metric for each big category\n",
    "    for i, category in enumerate(unique_cats):\n",
    "        # take just a certain category\n",
    "        cat_idx = np.where(categories == category)[0]\n",
    "        y_pred_cat = y_pred[cat_idx]\n",
    "        y_true_cat = y_true[cat_idx]\n",
    "\n",
    "        # if there is no matches in the category then PRAUC=0\n",
    "        if sum(y_true_cat) == 0:\n",
    "            pr_aucs.append(0)\n",
    "            weights.append(counts[i] / len(categories))\n",
    "            continue\n",
    "        \n",
    "        # get coordinates (x, y) for (recall, precision) of PR-curve\n",
    "        y, x, _ = precision_recall_curve(y_true_cat, y_pred_cat)\n",
    "        \n",
    "        # reverse the lists so that x's are in ascending order (left to right)\n",
    "        y = y[::-1]\n",
    "        x = x[::-1]\n",
    "        \n",
    "        # get indices for x-coordinate (recall) where y-coordinate (precision) \n",
    "        # is higher than precision level (75% for our task)\n",
    "        good_idx = np.where(y >= prec_level)[0]\n",
    "        \n",
    "        # if there are more than one such x's (at least one is always there, \n",
    "        # it's x=0 (recall=0)) we get a grid from x=0, to the rightest x \n",
    "        # with acceptable precision\n",
    "        if len(good_idx) > 1:\n",
    "            gt_prec_level_idx = np.arange(0, good_idx[-1] + 1)\n",
    "        # if there is only one such x, then we have zeros in the top scores \n",
    "        # and the curve simply goes down sharply at x=0 and does not rise \n",
    "        # above the required precision: PRAUC=0\n",
    "        else:\n",
    "            pr_aucs.append(0)\n",
    "            weights.append(counts[i] / len(categories))\n",
    "            continue\n",
    "        \n",
    "        # calculate category weight anyway\n",
    "        weights.append(counts[i] / len(categories))\n",
    "        # calculate PRAUC for all points where the rightest x \n",
    "        # still has required precision \n",
    "        try:\n",
    "            pr_auc_prec_level = auc(x[gt_prec_level_idx], y[gt_prec_level_idx])\n",
    "            if not np.isnan(pr_auc_prec_level):\n",
    "                pr_aucs.append(pr_auc_prec_level)\n",
    "        except ValueError:\n",
    "            pr_aucs.append(0)\n",
    "            \n",
    "    return np.average(pr_aucs, weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.6737361076111674\n",
      "3 0.08432013986586123\n",
      "4 0.675490557871805\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "for fold in range(2, 5):\n",
    "\n",
    "    features = pd.read_parquet(f\"/root/fold_{fold}_test.parquet\")\n",
    "\n",
    "    features[\"text\"] = features[\"name1\"] + \"[SEP]\" + features[\"name2\"]\n",
    "    features[\"label\"] = features[\"target\"].astype(int)\n",
    "    features = features.loc[features[[\"text\", \"label\"]].drop_duplicates().index]\n",
    "\n",
    "    features = features.reset_index(drop=True)\n",
    "    test = features\n",
    "    \n",
    "    pipe = pipeline(\"text-classification\", f\"/predefined_classes_best_{fold}\", device=0, batch_size=32)\n",
    "    preds = pipe(list(test[\"text\"].values))\n",
    "    preds = [p[\"score\"] if p[\"label\"] else 1 - p[\"score\"] for p in preds]\n",
    "    test[\"scores\"] = preds\n",
    "    print(fold, pr_auc_macro(test))\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
